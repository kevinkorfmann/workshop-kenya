{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp session_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Session - Introduction to Deep Learning and Dataset Generation\n",
    "\n",
    "Deep Learning (DL) is a subset of machine learning that involves the use of neural networks with multiple layers. The concept of neural networks has been around since the 1940s, but it wasn't until the 1980s that DL gained traction with the development of backpropagation, a technique for training neural networks.\n",
    "\n",
    "Despite this progress, DL remained a niche area of research until the early 2000s, when advances in computing power, data storage, and algorithms led to a resurgence of interest in the field. In particular, the development of graphics processing units (GPUs) and the availability of large datasets, such as ImageNet, enabled researchers to train deep neural networks with many layers.\n",
    "\n",
    "Over the past decade, DL has emerged as a powerful tool for solving a wide range of problems in fields as diverse as computer vision, natural language processing, and robotics. DL has achieved state-of-the-art performance on tasks such as object recognition, speech recognition, machine translation, and playing complex games like Go and chess or Population Genetics.\n",
    "\n",
    "> A brief introduction to DL related to the field of Population Genetics is summarised by [Korfmann et al. (2023)](https://academic.oup.com/gbe/article/15/2/evad008/6997869).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this session is provide an intuition of what is it like to develop and apply neural networks using PyTorch, a popular deep learing framework."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 DL/Machine Learning Paradigm\n",
    "\n",
    "Machine learning (ML) and deep learning (DL) are two subfields of artificial intelligence (AI) that aim to enable computers to learn from data and improve their performance on a specific task without being explicitly programmed. The general paradigm of ML and DL involves the following steps:\n",
    "\n",
    "1. Data collection: The first step is to gather the data that will be used to train the machine learning algorithm. This can involve collecting data from various sources, such as sensors, databases, or the internet.\n",
    "\n",
    "2. Data preprocessing: The collected data needs to be cleaned, transformed, and prepared for the machine learning algorithm. This step includes tasks such as removing irrelevant or redundant data, handling missing values, and scaling the features.\n",
    "\n",
    "3. Model selection: The next step is to select a machine learning model that is appropriate for the task at hand. There are many different types of machine learning models, such as linear regression, decision trees, and neural networks, and the choice depends on the specific problem.\n",
    "\n",
    "4. Model training: In this step, the selected machine learning model is trained on the preprocessed data. The goal is to find the optimal parameters that minimize the error between the predicted outputs and the actual outputs.\n",
    "\n",
    "5. Model evaluation: Once the machine learning model is trained, it needs to be evaluated to measure its performance on new, unseen data. This step helps to ensure that the model is not overfitting to the training data and can generalize well to new data.\n",
    "\n",
    "6. Model application: Finally, the machine learning model can be deployed in a real-world environment where it can be used to make predictions or decisions. \n",
    "\n",
    "Deep learning is a specific type of machine learning that uses neural networks with many layers to learn complex representations of the input data. The general paradigm of deep learning is similar to that of machine learning, but it typically involves larger datasets, more complex models, and longer training times. Deep learning models often require specialized hardware, such as graphics processing units (GPUs), to train efficiently."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Deep learning models\n",
    "\n",
    "There are several types of deep learning models that are used for different tasks and on different types of data. The basic building block of most deep learning models is the neural network, which consists of layers of interconnected nodes (neurons) that perform mathematical operations on the input data to produce an output.\n",
    "\n",
    "Here are some of the most common types of deep learning models:\n",
    "\n",
    "1. Convolutional Neural Networks (CNNs): CNNs are a type of neural network that is used for image classification, object detection, and other computer vision tasks. They are designed to process data that has a grid-like structure, such as images, and use convolutional layers to extract features from the input. The input to a CNN is a tensor of shape (width, height, channels), where channels correspond to the color channels of the image (e.g. red, green, blue).\n",
    "\n",
    "2. Recurrent Neural Networks (RNNs): RNNs are a type of neural network that is used for sequential data, such as time series data or natural language processing. They use recurrent layers to maintain a memory of previous inputs, which allows them to model temporal dependencies in the data. The input to an RNN is a sequence of vectors, where each vector corresponds to a timestep in the sequence.\n",
    "\n",
    "3. Long Short-Term Memory (LSTM) Networks: LSTMs are a type of RNN that is specifically designed to handle long-term dependencies in the input data. They use a special type of recurrent layer that can selectively remember or forget previous inputs. They have often been used for natural language processing tasks such as language modeling or machine translation.\n",
    "\n",
    "4. Generative Adversarial Networks (GANs): GANs are a type of neural network that is used for generative tasks, such as image synthesis and text generation. They consist of two neural networks that are trained in a game-like setting, with one network generating fake data and the other network trying to distinguish between real and fake data.\n",
    "\n",
    "5. Autoencoders: Autoencoders are a type of neural network that is used for unsupervised learning and dimensionality reduction. They are designed to encode the input data into a lower-dimensional representation and then decode it back into the original form, with the goal of minimizing the reconstruction error.\n",
    "\n",
    "6. Transformers: Transformers are a type of neural network that is used for natural language processing tasks, such as language translation and text summarization. They use self-attention mechanisms to process input sequences and can model long-range dependencies in the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Training loop\n",
    "\n",
    "In deep learning, the model is typically a neural network with many layers, and the training loop involves adjusting the weights and biases of these layers to minimize the error between the predicted output and the true output. The training loop consists of the following steps:\n",
    "\n",
    "1. Data preprocessing: The first step in the training loop is to preprocess the input data to prepare it for use in the neural network. This may involve tasks such as normalization, scaling, or data augmentation.\n",
    "\n",
    "2. Forward pass: The next step is to perform a forward pass through the neural network using the input data. The output of each layer is passed as input to the next layer, and the process continues until the final layer produces the predicted output.\n",
    "\n",
    "3. Compute loss: The predicted output is compared to the true output to compute the loss or error between them. The loss function is a measure of how well the model is performing on the training data.\n",
    "\n",
    "4. Backward pass: The backward pass, also known as backpropagation, involves computing the gradients of the loss with respect to the weights and biases of the neural network. This is done using the chain rule of calculus to propagate the error backwards through the network.\n",
    "\n",
    "5. Update weights and biases: The gradients computed in the backward pass are used to update the weights and biases of the neural network. This is done using an optimization algorithm such as stochastic gradient descent (SGD), which adjusts the weights and biases in the direction that minimizes the loss function.\n",
    "\n",
    "6. Repeat: The above steps are repeated for a fixed number of iterations or until the model achieves a satisfactory level of performance on the training data. In practice, the training loop is often run for many epochs, with the entire training dataset passed through the network multiple times.\n",
    "\n",
    "7. Evaluation: After training is complete, the model is evaluated on a separate test dataset to measure its performance on new, unseen data.\n",
    "\n",
    "The above steps constitute the basic training loop for deep learning. In practice, there are many variations and extensions to this loop, such as regularization, early stopping, and learning rate scheduling, all of which can improve the performance of the model and make the training process more efficient."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Example for inference of balancing selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prerequisits - Balancing selection:\n",
    "\n",
    "Balancing selection is a type of natural selection that maintains genetic diversity in a population by favoring the maintenance of multiple alleles at a particular locus. This can occur through a variety of mechanisms, such as heterozygote advantage, frequency-dependent selection, or spatially variable selection. Balancing selection is in contrast to directional selection, which favors the fixation of a single allele over time.\n",
    "\n",
    "One type of balancing selection is overdominance, also known as heterozygote advantage. Overdominance occurs when the heterozygote genotype has a higher fitness than either of the homozygote genotypes. This can occur, for example, when different alleles at a particular locus confer resistance to different diseases or environmental stressors, or when different alleles have complementary functions. In these cases, individuals that are heterozygous for the locus have a selective advantage over individuals that are homozygous for either allele, leading to the maintenance of both alleles in the population.\n",
    "\n",
    "Overall, balancing selection is an important mechanism for maintaining genetic diversity in populations, and overdominance is one specific type of balancing selection that can lead to the maintenance of multiple alleles at a particular locus."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the steps of the training loop described above we show a simple example of inferring balancing selection. Is taken from the above, but a description of inference is task is provided below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"To illustrate the ability of deep learning to detect signals of recent balancing selection, we simulated a scenario inspired by available data in human population genetics. We simulated 2,000 50 kbp loci under either neutrality or overdominance (i.e. heterozygote advantage, a form of balancing selection) at the center of the locus, conditioned to a demographic model of European populations (Jouganous et al. 2017). We performed forward-in-time simulations using SLiM (Haller and Messer 2019), similarly to a previous study (Isildak et al. 2021). We imposed selection on a de novo mutation starting 10k years ago, with selection coefficients of 0.25% and 0.5%. We sampled 40 present-day haplotypes, and 10 ancient haplotypes at four different time points (8k, 4k, 2k, 1k years ago, mirroring a plausible human aDNA data collection).\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Coding start"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually we would start by setting up our simulator to train our training data, but in this case we already have simulations which we can use as training/testing dataset available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-learn\n",
    "# pip install chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "\n",
    "from workshop_kenya.msms2haplo import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"/home/kevinkorfmann/Projects/temporal-balancing-selection/temp-balance-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths(directories):\n",
    "    all_files = []\n",
    "    for d in directories:\n",
    "        path = Path(d)\n",
    "        files = os.listdir(path)\n",
    "        files = [path/file for file in files]\n",
    "        all_files.append(files)\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ancient_files_to_X(files):\n",
    "    Xs = []\n",
    "    for i in range(len(files[0])):\n",
    "        file_time0, file_time1, file_time2, file_time3 = files[0][i], files[1][i], files[2][i], files[3][i]\n",
    "        files_timepoints = [file_time0, file_time1, file_time2, file_time3]\n",
    "        matrices_timepoints = []\n",
    "        for file in files_timepoints:\n",
    "            chroms, positions = read_msms(filename=file, NCHROMS=10)\n",
    "            chroms = sort_min_diff(chroms)\n",
    "            matrices_timepoints.append(torch.Tensor(myresize(chroms)))\n",
    "        Xs.append(torch.vstack(matrices_timepoints))\n",
    "    X = torch.stack(Xs)\n",
    "    X = X.unsqueeze(1)\n",
    "    return X\n",
    "\n",
    "\n",
    "def recent_files_to_X(files):\n",
    "    Xs = []\n",
    "    for i in range(len(files[4])):\n",
    "        file_time = files[4][i]\n",
    "        files_timepoints = [file_time]\n",
    "        matrices_timepoints = []\n",
    "        for file in files_timepoints:\n",
    "            chroms, positions = read_msms(filename=file, NCHROMS=40)\n",
    "            chroms = sort_min_diff(chroms)\n",
    "            matrices_timepoints.append(torch.Tensor(myresize(chroms)))\n",
    "        Xs.append(torch.vstack(matrices_timepoints))\n",
    "    X = torch.stack(Xs)\n",
    "    X = X.unsqueeze(1)\n",
    "    return X\n",
    "\n",
    "def myresize(chroms):\n",
    "    chroms = torch.Tensor(chroms)\n",
    "    if chroms.shape[1] >= 40:\n",
    "        chroms = chroms[:,:40]\n",
    "    else:\n",
    "        snp_dim = chroms.shape[1]\n",
    "        addzeros = 40 - snp_dim -1\n",
    "        chroms = F.pad(chroms, (1, addzeros), \"constant\", 0)\n",
    "    return chroms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "directories =  [data_path/\"neutral/1\", data_path/\"neutral/2\", data_path/\"neutral/3\", data_path/\"neutral/4\", data_path/\"neutral/5\"]\n",
    "files_neutral = get_paths(directories)\n",
    "\n",
    "directories =  [data_path/\"dominance0025/1\", data_path/\"dominance0025/2\", data_path/\"dominance0025/3\", data_path/\"dominance0025/4\", data_path/\"dominance0025/5\"]\n",
    "files_dominance0025 = get_paths(directories)\n",
    "\n",
    "directories =  [data_path/\"dominance05/1\", data_path/\"dominance05/2\", data_path/\"dominance05/3\", data_path/\"dominance05/4\", data_path/\"dominance05/5\"]\n",
    "files_dominance05 = get_paths(directories)\n",
    "\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "num_samples = 1000\n",
    "\n",
    "Xa_neutral = ancient_files_to_X(files_neutral)\n",
    "ya_neutral = torch.Tensor([0]*num_samples).long()\n",
    "\n",
    "Xa_dominance0025 = ancient_files_to_X(files_dominance0025)\n",
    "ya_dominance0025 = torch.Tensor([1]*num_samples).long()\n",
    "\n",
    "Xa_dominance05 = ancient_files_to_X(files_dominance05)\n",
    "ya_dominance05 = torch.Tensor([2]*num_samples).long()\n",
    "\n",
    "Xa = torch.cat([Xa_neutral, Xa_dominance05, Xa_dominance0025], axis=0)\n",
    "ya = torch.cat([ya_neutral, ya_dominance05, ya_dominance0025], axis=0)\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X (ancient) torch.Size([3000, 1, 40, 40])\n",
      "Shape of y (ancient) torch.Size([3000])\n"
     ]
    }
   ],
   "source": [
    "#print(f\"Shape of X (ancient) {Xa.shape}\")\n",
    "#print(f\"Shape of y (ancient) {ya.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "num_samples = 1000\n",
    "\n",
    "Xr_neutral = recent_files_to_X(files_neutral)\n",
    "yr_neutral = torch.Tensor([0]*num_samples).long()\n",
    "\n",
    "Xr_dominance05 = recent_files_to_X(files_dominance05)\n",
    "yr_dominance05 = torch.Tensor([2]*num_samples).long()\n",
    "\n",
    "Xr_dominance0025 = recent_files_to_X(files_dominance0025)\n",
    "yr_dominance0025 = torch.Tensor([1]*num_samples).long()\n",
    "\n",
    "Xr = torch.cat([Xr_neutral, Xr_dominance05, Xr_dominance0025], axis=0)\n",
    "yr = torch.cat([yr_neutral, yr_dominance05, yr_dominance0025], axis=0)\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X (ancient) torch.Size([3000, 1, 40, 40])\n",
      "Shape of y (ancient) torch.Size([3000])\n"
     ]
    }
   ],
   "source": [
    "# print(f\"Shape of X (ancient) {Xr.shape}\")\n",
    "# print(f\"Shape of y (ancient) {yr.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = np.random.choice(range(0,3000), 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancingSelectionDataset(Dataset):\n",
    "    \"\"\"BalancingSelectionDataset dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, Xa, Xr, ya, yr):\n",
    "        self.Xa = Xa\n",
    "        self.ya = ya\n",
    "        self.Xr = Xr\n",
    "        self.yr = yr\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.Xa.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.Xa[idx], self.Xr[idx], self.ya[idx], self.yr[idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(Xa, Xr, ya, yr):\n",
    "    \n",
    "    split = 1000\n",
    "    bs_val_dataset = BalancingSelectionDataset(Xa[random_idx[0:split]], Xr[random_idx[0:split]], ya[random_idx[0:split]], yr[random_idx[0:split]])\n",
    "    \n",
    "    bs_train_dataset = BalancingSelectionDataset(Xa[random_idx[split:]], Xr[random_idx[split:]], ya[random_idx[split:]], yr[random_idx[split:]])\n",
    "    \n",
    "    dataset_sizes = {}\n",
    "    dataset_sizes[\"train\"] = len(bs_train_dataset)\n",
    "    dataset_sizes[\"val\"] = len(bs_val_dataset)\n",
    "    train_dl = torch.utils.data.DataLoader(bs_train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "    val_dl = torch.utils.data.DataLoader(bs_val_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "    dataloaders = {\"train\":train_dl, \"val\":val_dl}\n",
    "    return dataloaders, dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for Xa, Xr, ya, yr in dataloaders[phase]:\n",
    "                \n",
    "                Xa = Xa.to(device)\n",
    "                Xr = Xr.to(device)\n",
    "                ya = ya.to(device)\n",
    "                yr = yr.to(device)\n",
    "                \n",
    "                #assert ya == yr\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(Xa, Xr)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, ya)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * Xa.size(0)\n",
    "                running_corrects += torch.sum(preds == ya.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoBranchEnsembleModel(nn.Module):\n",
    "    def __init__(self, ancient_model, recent_model, nb_classes=2):\n",
    "        super(TwoBranchEnsembleModel, self).__init__()\n",
    "        \n",
    "        self.ancient_model = ancient_model\n",
    "        self.recent_model = recent_model\n",
    "        \n",
    "        input_size = num_output_layers_single_model+num_output_layers_single_model\n",
    "        \n",
    "        self.l0 = nn.Linear(input_size, input_size//2)\n",
    "        self.l1 = nn.Linear(input_size//2, input_size//4)\n",
    "        self.l2 = nn.Linear(input_size//4, nb_classes)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, ancient_data, recent_data):\n",
    "        x0 = self.ancient_model(ancient_data)  \n",
    "        x1 = self.recent_model(recent_data) \n",
    "        \n",
    "        x = torch.cat((x0, x1), dim=1)\n",
    "        \n",
    "        x = self.l0(F.relu(x))\n",
    "        x = self.l1(F.relu(x))\n",
    "        x = self.l2(F.relu(x))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "def create_convnet(num_output_layers=2):\n",
    "    model = models.resnet18(pretrained=False)    \n",
    "    model.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_output_layers)\n",
    "    return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "confusion_matrices_both_recent_ancient = []\n",
    "\n",
    "for _ in tqdm(range(3)):\n",
    "\n",
    "    # get dataloaders\n",
    "    dataloaders, dataset_sizes = get_dataloader(Xa, Xr, ya, yr)\n",
    "    \n",
    "    # prepare ensemble model\n",
    "    num_output_layers_single_model = 64\n",
    "    ancient_model = create_convnet(num_output_layers=num_output_layers_single_model)\n",
    "    recent_model = create_convnet(num_output_layers=num_output_layers_single_model)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = TwoBranchEnsembleModel(ancient_model, recent_model, nb_classes=3).to(device)\n",
    "\n",
    "    # prepare training\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_ft = optim.Adam(model.parameters(), lr=0.0005)\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "    \n",
    "    # train for 3 epochs\n",
    "    model = ensemble_train_model(model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=4)\n",
    "\n",
    "\n",
    "    # validate\n",
    "    y_hat = []\n",
    "    y_true = []\n",
    "    with torch.no_grad():\n",
    "        for Xba, Xbr, yba, ybr in dataloaders[\"val\"]:\n",
    "            Xba = Xba.to(device)\n",
    "            Xbr = Xbr.to(device)\n",
    "            yba = yba.to(device)\n",
    "            ybr = ybr.to(device)\n",
    "            outputs = model(Xba, Xbr)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            y_hat += preds.cpu().numpy().tolist()\n",
    "            y_true += yba.cpu().numpy().tolist()\n",
    "\n",
    "\n",
    "    # save confusion\n",
    "    cm = confusion_matrix(y_true, y_hat)\n",
    "    cm = cm / cm.sum(1)\n",
    "\n",
    "    confusion_matrices_both_recent_ancient.append(cm)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(confusion_matrices, ax):\n",
    "\n",
    "    bootstrap_cm = np.array(confusion_matrices).mean(0)\n",
    "    bootstrap_stds = np.array(confusion_matrices).std(0)\n",
    "\n",
    "    # standard deviation labels\n",
    "    accuracies = [str(np.round(v, 2)) for v in bootstrap_cm.flatten().tolist()]\n",
    "    stds = [str(np.round(v, 2)) for v in bootstrap_stds.flatten().tolist()]\n",
    "    labels = [a  + \"±\\n\" + s for a, s in zip(accuracies, stds)]\n",
    "    labels = np.array(labels).reshape(3,3)\n",
    "    \n",
    "    sns.heatmap(bootstrap_cm, annot = labels, fmt = '', cmap=\"Blues\", ax=ax, xticklabels=[\"N\", \"D0.25\", \"D0.5\"], yticklabels=[\"N\", \"D0.25\", \"D0.5\"], linecolor='black', linewidths=.5)\n",
    "\n",
    "    ax.axhline(y=0, color='k',linewidth=5)\n",
    "    ax.axhline(y=3, color='k',linewidth=5)\n",
    "    ax.axvline(x=0, color='k',linewidth=5)\n",
    "    ax.axvline(x=3, color='k',linewidth=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import seaborn as sns \n",
    "\n",
    "sns.set(style=\"white\", font_scale=1.75)\n",
    "\n",
    "fig, ax = plt.subplots(1 , sharex=True, sharey=True, figsize=(7, 7))\n",
    "plot_confusion_matrix(confusion_matrices_both_recent_ancient, ax)\n",
    "ax.set_title(\"ancient+recent\")\n",
    "\n",
    "#plt.savefig(\"./images/confusion.png\", dpi=250)\n",
    "````"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:500px; height:500px\">\n",
    "\n",
    "![Confusion matrix](./images/confusion.png) \n",
    "</div>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
